{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333b0698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"  # Set JAVA_HOME to your Java installation path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from konlpy.tag import Mecab\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1d194-beb8-4730-859e-5c405d582808",
   "metadata": {},
   "source": [
    "# Bridging Languages with Deep Learning: Building a Korean-English Translator (with beam search)\n",
    "#### **By Eunice Tu & Yewon Park**\n",
    "\n",
    "In this project, we build a Neural Machine Translation (NMT) system to translate Korean text into English using deep learning models. We experiment with two main architectures: an LSTM-based Sequence-to-Sequence (Seq2Seq) model with attention, and a Transformer-based model. Our aim is to develop models that produce fluent, accurate translations that outperform traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532512b-148f-4314-950c-6e195c5a22e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Source:\n",
    "We used the AI Hub Korean-English Parallel Corpus, a professionally curated dataset provided by the Korean government. It contains over one million aligned Korean-English sentence pairs across diverse domains such as news articles, spoken conversations, legal documents, IT, and patents. This dataset is well-suited for our translation project because:\n",
    "\n",
    "- It provides clean, high-quality translations covering both formal and informal language.\n",
    "\n",
    "- It offers sufficient volume to train deep learning models effectively.\n",
    "\n",
    "- It reflects a variety of real-world contexts, improving the model's generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66407b6-417f-4d5d-8df5-ee768a49c1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Setup and Configuration\n",
    "\n",
    "This function sets up the computing environment to ensure consistent and reproducible results across different runs. It includes GPU configuration, seed initialization, and hardware information logging.\n",
    "\n",
    "- Detects GPU availability using PyTorch and sets the device accordingly\n",
    "- Configures deterministic behavior for consistent performance (optional)\n",
    "- Sets random seeds for PyTorch, NumPy, and Python's `random` module\n",
    "- Prints device and memory information when using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d421da9-1d51-423b-9322-bc1b68fb66d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initial setup, device configuration, and random seed settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = False  # Avoid random spikes\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc618589-4cbb-4064-9f9a-9b914e36f594",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preprocessing\n",
    "\n",
    "#### Initial Preparation\n",
    "To facilitate understanding and further analysis, the original column names in Korean need to be translated into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6128decc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved translated file: translated/1_spoken(1)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/1_spoken(2)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/2_conversation_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/3_news(1)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/3_news(2)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/3_news(3)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/3_news(4)_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/4_korean_culture_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/5_decree_200226_translated.xlsx\n",
      "✅ Saved translated file: translated/6_government_website_200226_translated.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Base folder where original files are\n",
    "base_folder = \"AIHub_translation\"\n",
    "\n",
    "# Folder where translated files will be saved\n",
    "translated_folder = \"translated\"\n",
    "os.makedirs(translated_folder, exist_ok=True)\n",
    "\n",
    "files = [\n",
    "    \"1_spoken(1)_200226.xlsx\",\n",
    "    \"1_spoken(2)_200226.xlsx\",\n",
    "    \"2_conversation_200226.xlsx\",\n",
    "    \"3_news(1)_200226.xlsx\",\n",
    "    \"3_news(2)_200226.xlsx\",\n",
    "    \"3_news(3)_200226.xlsx\",\n",
    "    \"3_news(4)_200226.xlsx\",\n",
    "    \"4_korean_culture_200226.xlsx\",\n",
    "    \"5_decree_200226.xlsx\",\n",
    "    \"6_government_website_200226.xlsx\"\n",
    "]\n",
    "\n",
    "translated_files = [\n",
    "    \"1_spoken(1)_200226_translated.xlsx\",\n",
    "    \"1_spoken(2)_200226_translated.xlsx\",\n",
    "    \"2_conversation_200226_translated.xlsx\",\n",
    "    \"3_news(1)_200226_translated.xlsx\",\n",
    "    \"3_news(2)_200226_translated.xlsx\",\n",
    "    \"3_news(3)_200226_translated.xlsx\",\n",
    "    \"3_news(4)_200226_translated.xlsx\",\n",
    "    \"4_korean_culture_200226_translated.xlsx\",\n",
    "    \"5_decree_200226_translated.xlsx\",\n",
    "    \"6_government_website_200226_translated.xlsx\"\n",
    "]\n",
    "\n",
    "# Translating the column names from Korean to English\n",
    "col_translation = {\n",
    "    'SID': 'sid',\n",
    "    'ID': 'id',\n",
    "    '원문': 'korean',\n",
    "    '번역문': 'english',\n",
    "    '대분류': 'main_category',\n",
    "    '소분류': 'sub_category',\n",
    "    '상황': 'situation',\n",
    "    'Set Nr.': 'set_number',\n",
    "    '발화자': 'speaker',\n",
    "    '날짜': 'date',\n",
    "    '자동분류1': 'auto_category1',\n",
    "    '자동분류2': 'auto_category2',\n",
    "    '자동분류3': 'auto_category3',\n",
    "    'URL': 'url',\n",
    "    '언론사': 'media',\n",
    "    '키워드': 'keyword',\n",
    "    '지자체': 'local_government'\n",
    "}\n",
    "\n",
    "for f in files:\n",
    "    path = os.path.join(base_folder, f)\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "\n",
    "    # Translate column names\n",
    "    translated_columns = {col: col_translation.get(col, col) for col in df.columns}\n",
    "    df.rename(columns=translated_columns, inplace=True)\n",
    "\n",
    "    # Save to new translated folder\n",
    "    save_path = os.path.join(translated_folder, f.replace(\".xlsx\", \"_translated.xlsx\"))\n",
    "    df.to_excel(save_path, index=False)\n",
    "\n",
    "    print(f\"✅ Saved translated file: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2aa4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>situation</th>\n",
       "      <th>set_number</th>\n",
       "      <th>speaker</th>\n",
       "      <th>korean</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>1</td>\n",
       "      <td>A-1</td>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
       "      <td>How is the market's reaction to the newly rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>1</td>\n",
       "      <td>B-1</td>\n",
       "      <td>판매량이 지난번 제품보다 빠르게 늘고 있습니다.</td>\n",
       "      <td>The sales increase is faster than the previous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>1</td>\n",
       "      <td>A-2</td>\n",
       "      <td>그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.</td>\n",
       "      <td>Then, we'll have to call the manufacturer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>1</td>\n",
       "      <td>B-2</td>\n",
       "      <td>네, 제가 연락해서 주문량을 2배로 늘리겠습니다.</td>\n",
       "      <td>Sure, I'll make a call and double the volume o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>2</td>\n",
       "      <td>A-1</td>\n",
       "      <td>지난 회의 마지막에 논의했던 안건을 다시 볼까요?</td>\n",
       "      <td>Shall we take a look at the issues we discusse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category sub_category situation  set_number speaker  \\\n",
       "0          비즈니스           회의   의견 교환하기           1     A-1   \n",
       "1          비즈니스           회의   의견 교환하기           1     B-1   \n",
       "2          비즈니스           회의   의견 교환하기           1     A-2   \n",
       "3          비즈니스           회의   의견 교환하기           1     B-2   \n",
       "4          비즈니스           회의   의견 교환하기           2     A-1   \n",
       "\n",
       "                         korean  \\\n",
       "0   이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
       "1    판매량이 지난번 제품보다 빠르게 늘고 있습니다.   \n",
       "2  그렇다면 공장에 연락해서 주문량을 더 늘려야겠네요.   \n",
       "3   네, 제가 연락해서 주문량을 2배로 늘리겠습니다.   \n",
       "4   지난 회의 마지막에 논의했던 안건을 다시 볼까요?   \n",
       "\n",
       "                                             english  \n",
       "0  How is the market's reaction to the newly rele...  \n",
       "1  The sales increase is faster than the previous...  \n",
       "2  Then, we'll have to call the manufacturer and ...  \n",
       "3  Sure, I'll make a call and double the volume o...  \n",
       "4  Shall we take a look at the issues we discusse...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the translated files\n",
    "path = os.path.join(translated_folder, translated_files[2])\n",
    "df = pd.read_excel(path, engine='openpyxl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970d8bb-e94d-420a-bbd9-d6405ef53513",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "\n",
    "To ensure that the input data is clean, standardized, and properly formatted for efficient model training, we applied the following preprocessing steps:\n",
    "\n",
    "- **Normalization:**  \n",
    "  We first converted all text to lowercase and standardized spacing by adding spaces around punctuation marks such as `.`, `,`, `?`, and `!`. Non-alphabetic characters (except essential punctuation) were removed, and any multiple consecutive spaces were collapsed into a single space. For robustness, the function also returns an empty string for non-string inputs, ensuring that edge cases are handled appropriately.\n",
    "\n",
    "- **Tokenization:**  \n",
    "  After normalization, we applied a simple word-level tokenizer. Each unique word is assigned a unique index, and special tokens — `<pad>`, `<sos>`, `<eos>`, and `<unk>` — were included to manage padding, sequence start, sequence end, and unknown words respectively.\n",
    "\n",
    "- **Sequence Preparation:**  \n",
    "  Each sentence was wrapped with `<sos>` (start of sentence) and `<eos>` (end of sentence) tokens. Sequences were either padded or truncated to a fixed maximum length, allowing for efficient batching during model training.\n",
    "\n",
    "- **Data Splitting:**  \n",
    "  Currently, the entire dataset is being used for model training. In future work, we plan to split the dataset into training, validation, and test sets to better evaluate the model’s generalization performance.\n",
    "\n",
    "These preprocessing steps collectively transform raw text into a structured format that is clean, consistent, and optimized for training effective machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f38fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_english(sentence):\n",
    "    \"\"\"Clean English: lowercase, remove symbols, keep letters.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[^a-zA-Z?.!,\\s]', '', sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_korean(sentence):\n",
    "    \"\"\"For Korean, just trim extra spaces (DO NOT strip Hangul).\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.strip()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ff6a6-ce6b-46fd-816e-0811c451378e",
   "metadata": {},
   "source": [
    "#### Tokenizer Class\n",
    "The ImprovedTokenizer class provides a custom way to convert text into token IDs and back, enabling consistent text preprocessing and decoding.:\n",
    "\n",
    "- Initialized with a vocabulary size limit \n",
    "- Contains special tokens: `<pad>`(0), `<sos>`(1), `<eos>`(2), and `<unk>`(3)\n",
    "- Builds vocabulary based on word frequency in the training data\n",
    "- The most common words are included in the vocabulary up to the max limit\n",
    "- Provides methods to encode sentences into token IDs and decode IDs back to text\n",
    "- Implements save/load functionality for persistence between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad763aa-17db-4ab0-93fa-9a38e2efdeca",
   "metadata": {},
   "source": [
    "`ImprovedKoreanTokenizer`\n",
    "\n",
    "This tokenizer provides character-level support for Korean text:\n",
    "\n",
    "- The `tokenize()` method uses `mecab.morphs()` to properly segment Korean text into morphological units\\\n",
    "    **Morphological units (or morphemes) are the smallest linguistic units in a language that have meaning or grammatical function.**\n",
    "- During vocabulary building, it counts frequency of each morphological unit\n",
    "- Words are sorted by frequency and limited to the maximum vocabulary size\n",
    "- The `encode()` method converts Korean text into token IDs using the built vocabulary\n",
    "- The `decode()` method converts token IDs back into readable Korean text\n",
    "\n",
    "**📌 Why Morphological Analysis Matters for Korean**\n",
    "\n",
    "The ImprovedKoreanTokenizer uses MeCab (through the mecab.morphs() function) to perform morphological analysis because:\n",
    "\n",
    "- Korean is an agglutinative language where numerous grammatical elements attach to word stems\n",
    "- Simply splitting by spaces would be ineffective as Korean sentences often have many morphemes within a single space-separated \"word\"\n",
    "- Character-level splitting would lose the semantic meaning carried by morphological units\n",
    "- Proper morphological segmentation provides much better input for NLP tasks like translation or sentiment analysis\n",
    "\n",
    "This is why the Korean tokenizer needs specialized processing while the English tokenizer can rely on simpler space-based splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16585f81-1bfb-4fa7-91f1-d563e7abf056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Mecab tokenizer for Korean\n",
    "mecab = Mecab()\n",
    "def tokenize_korean(text):\n",
    "    return mecab.morphs(text)\n",
    "\n",
    "class ImprovedKoreanTokenizer:\n",
    "    \"\"\"Korean-specific tokenizer with character-level support\"\"\"\n",
    "    def __init__(self, max_vocab=50000):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.word_freq = {}\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"Tokenize Korean text appropriately\"\"\"\n",
    "        return tokenize_korean(sentence)\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences\"\"\"\n",
    "        for sent in tqdm(sentences, desc=\"Building vocabulary\"):\n",
    "            for word in self.tokenize(sent):\n",
    "                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Sort by frequency and limit vocab size\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_limit = min(len(sorted_words), self.max_vocab - 4)\n",
    "\n",
    "        idx = 4\n",
    "        for word, _ in sorted_words[:vocab_limit]:\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        \"\"\"Convert sentence to token IDs\"\"\"\n",
    "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in self.tokenize(sentence)]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        return ' '.join([self.idx2word.get(idx, \"<unk>\") for idx in indices if idx != self.word2idx[\"<pad>\"]])\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer state\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load tokenizer state\"\"\"\n",
    "        obj = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            obj.__dict__.update(pickle.load(f))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c12291-eb9b-4dcc-99fb-24d29588c875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnglishTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer for English\"\"\"\n",
    "    def __init__(self, max_vocab=30000):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.word_freq = {}\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"Split English text into words\"\"\"\n",
    "        return word_tokenize(sentence)\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        for sent in tqdm(sentences, desc=\"Building vocabulary\"):\n",
    "            for word in self.tokenize(sent):\n",
    "                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n",
    "\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_limit = min(len(sorted_words), self.max_vocab - 4)\n",
    "\n",
    "        idx = 4\n",
    "        for word, _ in sorted_words[:vocab_limit]:\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in self.tokenize(sentence)]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(idx, \"<unk>\") for idx in indices if idx != self.word2idx[\"<pad>\"]])\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        obj = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            obj.__dict__.update(pickle.load(f))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961dee2d-9e34-4419-92cb-211dcb8843e7",
   "metadata": {},
   "source": [
    "#### Dataset Class with Caching\n",
    "The `CachedTranslationDataset` class is designed to improve training efficiency for machine translation tasks by preprocessing and caching tokenized data.\n",
    "\n",
    "- **Built on PyTorch Dataset:**  \n",
    "  Inherits from `torch.utils.data.Dataset`, making it fully compatible with PyTorch's `DataLoader` for efficient batching and shuffling.\n",
    "\n",
    "- **Caching Mechanism:**  \n",
    "  Preprocesses each data sample once and stores the result in memory, avoiding repeated preprocessing during each epoch and speeding up training.\n",
    "\n",
    "- **Tokenization and Sequence Preparation:**  \n",
    "  - Converts source sentences (e.g., Korean) and target sentences (e.g., English) into token ID sequences.\n",
    "  - Adds special tokens like `<sos>` (start of sentence) and `<eos>` (end of sentence).\n",
    "  - Applies padding or truncation to ensure all sequences have a fixed maximum length.\n",
    "\n",
    "- **Attention Mask Creation:**  \n",
    "  Generates attention masks that distinguish between actual tokens and padding tokens, helping the model ignore padded positions during training.\n",
    "\n",
    "- **Model-Ready Outputs:**  \n",
    "  Returns tensors for:\n",
    "  - Source input IDs and attention masks\n",
    "  - Target input IDs and labels  \n",
    "  These tensors are ready to be directly fed into the model for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdf63e7-3dd4-4cf9-b07f-756b7fd7c6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CachedTranslationDataset(Dataset):\n",
    "    \"\"\"Dataset with caching for faster loading\"\"\"\n",
    "    def __init__(self, df, src_tokenizer, tgt_tokenizer, max_len=40, cache_dir='dataset_cache'):\n",
    "        self.df = df\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_file = os.path.join(cache_dir, f\"cached_data_{len(df)}_{max_len}.pkl\")\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Try to load from cache, otherwise process data\n",
    "        self.cached_data = self._load_or_create_cache()\n",
    "\n",
    "    def _load_or_create_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            print(f\"Loading cached dataset from {self.cache_file}\")\n",
    "            with open(self.cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        print(\"Creating new dataset cache...\")\n",
    "        cached_data = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df)), desc=\"Processing dataset\"):\n",
    "            src_text = self.df.iloc[idx]['korean']\n",
    "            tgt_text = self.df.iloc[idx]['english']\n",
    "            \n",
    "            src_seq = [1] + self.src_tokenizer.encode(src_text) + [2]  # <sos> + sentence + <eos>\n",
    "            tgt_seq = [1] + self.tgt_tokenizer.encode(tgt_text) + [2]  # <sos> + sentence + <eos>\n",
    "            \n",
    "            # Truncate sequences to max_len\n",
    "            src_seq = src_seq[:self.max_len]\n",
    "            tgt_seq = tgt_seq[:self.max_len]\n",
    "            \n",
    "            # Create source and target masks\n",
    "            src_mask = [1] * len(src_seq) + [0] * (self.max_len - len(src_seq))\n",
    "            tgt_mask = [1] * len(tgt_seq) + [0] * (self.max_len - len(tgt_seq))\n",
    "            \n",
    "            # Pad sequences\n",
    "            src_seq += [0] * (self.max_len - len(src_seq))\n",
    "            tgt_seq += [0] * (self.max_len - len(tgt_seq))\n",
    "            \n",
    "            cached_data.append({\n",
    "                'src': torch.tensor(src_seq, dtype=torch.long),\n",
    "                'tgt': torch.tensor(tgt_seq, dtype=torch.long),\n",
    "                'src_mask': torch.tensor(src_mask, dtype=torch.bool),\n",
    "                'tgt_mask': torch.tensor(tgt_mask, dtype=torch.bool),\n",
    "                'src_len': len(src_seq),\n",
    "                'tgt_len': len(tgt_seq)\n",
    "            })\n",
    "        \n",
    "        # Save to cache\n",
    "        print(f\"Saving dataset cache to {self.cache_file}\")\n",
    "        with open(self.cache_file, 'wb') as f:\n",
    "            pickle.dump(cached_data, f)\n",
    "        \n",
    "        return cached_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cached_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01da16c-9062-4d09-a52b-4fbbd67b7bd3",
   "metadata": {},
   "source": [
    "#### Data Loading Functions\n",
    "`load_excel_files`:\n",
    "- combines all Excel files from a specified directory into a single DataFrame \n",
    "- automatically identifies the Korean and English columns, preprocesses the text\n",
    "- caches the combined DataFrame for faster reloading in future runs.\n",
    "\n",
    "`create_dataloaders`:\n",
    "- Splits the dataset into training and validation sets\n",
    "- Allows optional subsampling to use only a fraction of the available data if needed. Finally \n",
    "- Creates PyTorch `DataLoader` objects to efficiently batch and feed the data to the model during training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94661f62-d794-4fcc-bbc8-45e7044f8d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_excel_files(directory='./translated', pattern=\"*translated.xlsx\"):\n",
    "    \"\"\"Load and combine all Excel files matching the pattern\"\"\"\n",
    "    all_data = []\n",
    "    cache_file = os.path.join(directory, \"combined_data_cache.pkl\")\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading combined data from cache: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    files = list(Path(directory).glob(pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files matching pattern '{pattern}' found in directory '{directory}'\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(files)} files matching the pattern\")\n",
    "    \n",
    "    for file_path in tqdm(files, desc=\"Loading Excel files\"):\n",
    "        try:\n",
    "            # Attempt to identify Korean and English columns based on common patterns\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Try to automatically detect Korean and English columns\n",
    "            korean_col = None\n",
    "            english_col = None\n",
    "            \n",
    "            # Common column name patterns\n",
    "            korean_patterns = ['korean', 'ko', '한국어', 'source']\n",
    "            english_patterns = ['english', 'en', '영어', 'target']\n",
    "            \n",
    "            # Check column names\n",
    "            for col in df.columns:\n",
    "                col_lower = str(col).lower()\n",
    "                if any(pattern in col_lower for pattern in korean_patterns):\n",
    "                    korean_col = col\n",
    "                if any(pattern in col_lower for pattern in english_patterns):\n",
    "                    english_col = col\n",
    "            \n",
    "            # If automatic detection fails, use the first two columns\n",
    "            if korean_col is None or english_col is None:\n",
    "                if len(df.columns) >= 2:\n",
    "                    korean_col = df.columns[0]\n",
    "                    english_col = df.columns[1]\n",
    "                    print(f\"Using columns: {korean_col} and {english_col} for {file_path.name}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {file_path.name}: Not enough columns\")\n",
    "                    continue\n",
    "            \n",
    "            # Extract and rename columns\n",
    "            file_data = df[[korean_col, english_col]].copy()\n",
    "            file_data.columns = ['korean', 'english']\n",
    "            \n",
    "            # Add source file information\n",
    "            file_data['source_file'] = file_path.name\n",
    "            \n",
    "            # Append to combined data\n",
    "            all_data.append(file_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Clean the data\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data['english'] = combined_data['english'].apply(preprocess_english)\n",
    "    combined_data['korean'] = combined_data['korean'].apply(preprocess_korean)\n",
    "    \n",
    "    # Remove rows with too short sentences\n",
    "    combined_data = combined_data[\n",
    "    (combined_data['english'].str.split().str.len() > 3) &\n",
    "    (combined_data['korean'].str.split().str.len() > 3)\n",
    "    ]\n",
    "\n",
    "    # Remove rows with empty strings\n",
    "    combined_data = combined_data[(combined_data['english'] != '') & (combined_data['korean'] != '')]\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving combined data to cache: {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(combined_data, f)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def create_dataloaders(data, ko_tokenizer, en_tokenizer, train_ratio=0.8, \n",
    "                       batch_size=64, max_len=40, num_workers=4, pin_memory=True, \n",
    "                       subset_fraction=0.3):  # Added subset_fraction parameter\n",
    "    \"\"\"Create train and validation DataLoaders with optimized settings\"\"\"\n",
    "    \n",
    "    # Apply subset sampling - NEW\n",
    "    if subset_fraction < 1.0:\n",
    "        sample_size = int(len(data) * subset_fraction)\n",
    "        data = data.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Using {subset_fraction*100}% of the data: {len(data)} samples\")\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    val_data = data.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Training data: {len(train_data)} samples\")\n",
    "    print(f\"Validation data: {len(val_data)} samples\")\n",
    "    \n",
    "    # Create datasets with caching\n",
    "    train_dataset = CachedTranslationDataset(train_data, ko_tokenizer, en_tokenizer, max_len, cache_dir='dataset_cache/train')\n",
    "    val_dataset = CachedTranslationDataset(val_data, ko_tokenizer, en_tokenizer, max_len, cache_dir='dataset_cache/val')\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a9de8-ab2c-413d-a7f1-74e3d40b5c0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Implementation\n",
    "#### Transformer-Based Model:\n",
    "`PositionalEncoding`:\n",
    "\n",
    "- Adds position-dependent signals to token embeddings since the Transformer has no inherent notion of order.\n",
    "- Uses fixed sine and cosine patterns based on position and embedding dimension\n",
    "\n",
    "`TransformerModel`:\n",
    "\n",
    "- Implements a full encoder-decoder Transformer architecture using PyTorch’s nn.Transformer.\n",
    "- Handles embedding, positional encoding, masking, and decoding logic.\n",
    "- Supports both training (forward) and inference (beam_search) with step-wise prediction.\n",
    "- `beam_search()` enables more fluent translations compared to greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e0a4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information using sine and cosine signals\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sin to even indices, cos to odd\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)  # Avoids updating during training\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17ce9eb-4692-490f-af43-3f5ade5176b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Full Transformer model for sequence-to-sequence translation\"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Embedding + positional encodings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Transformer backbone (encoder-decoder)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final linear projection to vocab size\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self._init_parameters()\n",
    "\n",
    "        # Store hyperparameters\n",
    "        self.d_model = d_model\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize model weights with Xavier uniform\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    def create_mask(self, src, tgt):\n",
    "        \"\"\"Create padding and look-ahead masks for source and target\"\"\"\n",
    "        src_padding_mask = (src == 0).to(device)\n",
    "        tgt_padding_mask = (tgt == 0).to(device)\n",
    "        tgt_len = tgt.size(1)\n",
    "\n",
    "        # Prevent target positions from seeing future tokens\n",
    "        tgt_attention_mask = torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.bool), diagonal=1).to(device)\n",
    "        return src_padding_mask, tgt_padding_mask, tgt_attention_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Standard forward pass for training\"\"\"\n",
    "        src_padding_mask, tgt_padding_mask, tgt_attention_mask = self.create_mask(src, tgt)\n",
    "\n",
    "        # Embed + add position encoding\n",
    "        src_embedded = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "\n",
    "        # Transformer forward\n",
    "        output = self.transformer(\n",
    "            src=src_embedded,\n",
    "            tgt=tgt_embedded,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_attention_mask\n",
    "        )\n",
    "\n",
    "        return self.output_layer(output)\n",
    "    def beam_search(self, src, src_tokenizer, tgt_tokenizer, beam_width=5, max_len=50):\n",
    "        \"\"\"Decodes translation using beam search for better quality than greedy\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Tokenize if raw input\n",
    "            if isinstance(src, str):\n",
    "                src_tokens = [1] + src_tokenizer.encode(src) + [2]\n",
    "                src = torch.tensor([src_tokens]).to(device)\n",
    "            else:\n",
    "                src = src.to(device)\n",
    "\n",
    "            src_padding_mask = (src == 0).to(device)\n",
    "            src_embedded = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "\n",
    "            # Encode source once\n",
    "            memory = self.transformer.encoder(src_embedded, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "            # Initialize beam with <sos> token\n",
    "            sequences = [([1], 0.0)]  # ([tokens], score)\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                all_candidates = []\n",
    "                for seq, score in sequences:\n",
    "                    tgt_input = torch.tensor([seq]).to(device)\n",
    "                    tgt_mask = torch.triu(torch.ones(len(seq), len(seq), dtype=torch.bool), diagonal=1).to(device)\n",
    "                    tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt_input) * math.sqrt(self.d_model))\n",
    "\n",
    "                    # Decode next step\n",
    "                    output = self.transformer.decoder(\n",
    "                        tgt=tgt_embedded,\n",
    "                        memory=memory,\n",
    "                        tgt_mask=tgt_mask,\n",
    "                        memory_key_padding_mask=src_padding_mask,\n",
    "                        tgt_key_padding_mask=(tgt_input == 0)\n",
    "                    )\n",
    "\n",
    "                    logits = self.output_layer(output[:, -1, :])\n",
    "                    probs = torch.log_softmax(logits, dim=-1)\n",
    "                    topk = torch.topk(probs, beam_width)\n",
    "\n",
    "                    # Expand each beam with top tokens\n",
    "                    for i in range(beam_width):\n",
    "                        token = topk.indices[0, i].item()\n",
    "                        new_seq = seq + [token]\n",
    "                        new_score = (score + topk.values[0, i].item()) / ((len(new_seq) + 1) ** 0.7)\n",
    "                        all_candidates.append((new_seq, new_score))\n",
    "\n",
    "                # Keep top-k candidates\n",
    "                sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "                # Stop early if all sequences ended with <eos>\n",
    "                if all(seq[-1] == 2 for seq, _ in sequences):\n",
    "                    break\n",
    "\n",
    "            best_seq = sequences[0][0]\n",
    "            if 2 in best_seq:\n",
    "                best_seq = best_seq[:best_seq.index(2)]\n",
    "\n",
    "            return tgt_tokenizer.decode(best_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cea07-63c5-41db-9873-6870e01e7bf4",
   "metadata": {},
   "source": [
    "#### LSTM + Attention-based Seq2Seq\n",
    "`EncoderRNN` (LSTM Encoder):\n",
    "\n",
    "- Takes the input (source language) sequence, embeds the tokens into dense vectors, and passes them through an LSTM network to capture contextual information.\n",
    "- Outputs both the full sequence of hidden states (for attention) and the final hidden and cell states (for initializing the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af43710f-a990-4550-9a90-53297ca1c994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"LSTM Encoder\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8923b-a426-499e-b876-c258dbbd4286",
   "metadata": {},
   "source": [
    "`AttentionDecoderRNN` (LSTM Decoder with Attention):\n",
    "- At each decoding step, it uses an attention mechanism to dynamically focus on different parts of the encoder's hidden states.\n",
    "- Combines the current embedded decoder input with the attention context, processes it through an LSTM, and predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4049e443-6225-41be-beab-fab75162bebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"LSTM Decoder with Attention\"\"\"\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = nn.Linear(hidden_dim + embed_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim + embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        hidden_broadcast = hidden[-1].unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        attn_weights = torch.bmm(hidden_broadcast, encoder_outputs.transpose(1, 2))  # (batch_size, 1, seq_len)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Context vector\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Concatenate context and embedding\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, hidden_dim + embed_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d9904-acf2-41a7-99b7-9af79ea72526",
   "metadata": {
    "tags": []
   },
   "source": [
    "`Seq2SeqModel` (Seq2Seq Wrapper):\n",
    "- A wrapper that connects the encoder and decoder together into a full translation model.\n",
    "- Defines the overall sequence-to-sequence forward pass, starting by encoding the source sequence and then decoding the target sequence one token at a time using teacher forcing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96b6296-da03-4751-9f03-de76e1a61c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    \"\"\"Wrapper for LSTM Encoder-Decoder with Attention\"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            input = tgt[:, t]  # Teacher forcing: next input is ground truth\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6db720-4ede-4c56-9d47-f7c60fc0ef82",
   "metadata": {},
   "source": [
    "## Methods\n",
    "`train_epoch`:\n",
    "\n",
    "- Trains the Transformer model for one full epoch using teacher forcing.\n",
    "- Uses mixed precision (via autocast and GradScaler) for memory efficiency.\n",
    "- Applies gradient clipping to prevent exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b0baef-e54b-4f28-9f85-59cd9bc2faf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, clip=1.0):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch using teacher forcing.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Transformer model\n",
    "        train_loader (DataLoader): Batches of (src, tgt) pairs\n",
    "        optimizer (Optimizer): Optimizer like Adam\n",
    "        criterion (Loss): CrossEntropyLoss\n",
    "        clip (float): Gradient clipping norm\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss over the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        src = batch['src'].to(device)  # Source sequence (input)\n",
    "        tgt = batch['tgt'].to(device)  # Target sequence (label)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]  # Inputs for decoder (excluding <eos>)\n",
    "        tgt_output = tgt[:, 1:]  # Ground truth for loss (excluding <sos>)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):  # Enable mixed precision\n",
    "            output = model(src, tgt_input)  # Forward pass\n",
    "            output = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backpropagate with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding updates\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Step optimizer and update scale\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (progress_bar.n + 1)})\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4deda8-f186-4fc3-ad94-538ce46fbe92",
   "metadata": {},
   "source": [
    "`evaluate`:\n",
    "\n",
    "- Evaluate model performance on validation data without updating weights\n",
    "\n",
    "**Function Workflow**:\n",
    "\n",
    "- Set the model to evaluation mode (model.eval()).\n",
    "- Initialize variables for total loss (epoch_loss) and BLEU scores (bleu_scores).\n",
    "- Loop through validation data:\n",
    "  - Make predictions with the model.\n",
    "  - Compute loss by comparing predictions to target values.\n",
    "  - Calculate BLEU score to evaluate prediction quality.\n",
    "  - Optionally print translation examples for inspection.\n",
    "\n",
    "**BLEU Score**:\n",
    "- **Purpose**: BLEU (Bilingual Evaluation Understudy) measures the quality of machine-generated text by comparing n-grams (sequences of words) in the predicted output with those in reference translations.\n",
    "- **Key Features**:\n",
    "  - **N-gram Precision**: It checks how many n-grams (e.g., unigrams, bigrams) from the predicted sentence match those in the reference sentence.\n",
    "  - **Brevity Penalty**: A penalty is applied if the predicted output is shorter than the reference, encouraging the model to produce more complete translations.\n",
    "  - **Smoothing**: Smoothing is used to adjust BLEU calculations in cases where the model produces rare n-grams not present in the reference translations, ensuring more stable scores.\n",
    "- **Usage**: BLEU is especially useful in tasks like machine translation, where we want to evaluate how well a machine-generated translation matches human-produced translations.\n",
    "- **Limitations**: BLEU focuses on surface-level n-gram overlap and doesn't capture the full meaning or fluency of a sentence, so it's less effective for evaluating overall sentence quality or semantic accuracy.\n",
    "  \n",
    "**Output**:\n",
    "- Returns the average loss and average BLEU score across the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728d5f33-d01c-4511-aae8-7d691e9f0da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, ko_tokenizer=None, en_tokenizer=None, print_examples=3):\n",
    "    \"\"\"\n",
    "    Evaluate the Transformer model on validation data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained Transformer model\n",
    "        val_loader (DataLoader): Validation data batches\n",
    "        criterion (Loss): Loss function (e.g. CrossEntropyLoss)\n",
    "        ko_tokenizer (Tokenizer): Korean tokenizer (for decoding)\n",
    "        en_tokenizer (Tokenizer): English tokenizer (for decoding)\n",
    "        print_examples (int): Number of examples to print\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, average_bleu_score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    bleu_scores = []\n",
    "    smooth = SmoothingFunction().method4  # BLEU smoothing for short sentences\n",
    "    examples_printed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
    "            if i >= 200:\n",
    "                break  # For speed: evaluate only on 200 batches\n",
    "\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]   # Decoder input (no <eos>)\n",
    "            tgt_output = tgt[:, 1:]   # Ground-truth prediction (no <sos>)\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                output = model(src, tgt_input)  # Forward pass\n",
    "                output_flat = output.contiguous().view(-1, output.shape[-1])\n",
    "                tgt_output_flat = tgt_output.contiguous().view(-1)\n",
    "\n",
    "                loss = criterion(output_flat, tgt_output_flat)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # --- BLEU Score Calculation + Sample Printing ---\n",
    "            for j in range(src.size(0)):\n",
    "                if examples_printed >= print_examples:\n",
    "                    break\n",
    "\n",
    "                # Run beam search for prediction\n",
    "                src_seq = src[j].unsqueeze(0)\n",
    "                pred_text = model.beam_search(src_seq, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "                # Decode ground-truth target\n",
    "                tgt_sentence = [w for w in tgt[j].tolist() if w not in [0, 1, 2]]  # Remove <pad>, <sos>, <eos>\n",
    "                ref_text = en_tokenizer.decode(tgt_sentence)\n",
    "\n",
    "                # Calculate BLEU score (ref vs pred)\n",
    "                bleu = sentence_bleu(\n",
    "                    [ref_text.split()],\n",
    "                    pred_text.split(),\n",
    "                    smoothing_function=smooth\n",
    "                )\n",
    "                bleu_scores.append(bleu)\n",
    "\n",
    "                # Print example translation\n",
    "                src_text = ko_tokenizer.decode([w for w in src[j].tolist() if w not in [0, 1, 2]])\n",
    "                print(f\"\\n--- Example {examples_printed + 1} ---\")\n",
    "                print(f\"Source (KO): {src_text}\")\n",
    "                print(f\"Target (EN): {ref_text}\")\n",
    "                print(f\"Prediction : {pred_text}\")\n",
    "                examples_printed += 1\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    print(f\"\\nAverage BLEU score on validation set: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return epoch_loss / max(i, 1)  # Avoid divide-by-zero if no batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e6953-7b16-47b5-9aca-7960fee13aad",
   "metadata": {},
   "source": [
    "`train_model`:\n",
    "\n",
    "- Trains the model across multiple epochs with checkpointing and early stopping\n",
    "\n",
    "**Function Workflow**:\n",
    "\n",
    "- Trains the model using train_epoch() and validates using evaluate().\n",
    "- Tracks the best validation loss and saves the corresponding model checkpoint.\n",
    "- Stops training early if the validation loss does not improve for a specified number of epochs (patience).\n",
    "\n",
    "Output:\n",
    "- Returns the best validation loss achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3730be3f-8da3-41ce-8289-b8eb4bf5ef59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "                ko_tokenizer=None, en_tokenizer=None,\n",
    "                num_epochs=20, patience=2, model_save_path='models'):\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)  # Ensure save directory exists\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize with infinity\n",
    "    patience_counter = 0          # Tracks how long validation loss hasn't improved\n",
    "    scaler = GradScaler()         # AMP support\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # === TRAIN ONE EPOCH ===\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        # === VALIDATION ===\n",
    "        val_loss = evaluate(\n",
    "            model, val_loader, criterion,\n",
    "            ko_tokenizer=ko_tokenizer,\n",
    "            en_tokenizer=en_tokenizer\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} | Time: {end_time - start_time:.2f}s\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # === SAVE BEST MODEL ===\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(model_save_path, 'best_model.pt'))\n",
    "            print(f\"✅ Saved new best model (val loss {val_loss:.4f})\")\n",
    "            patience_counter = 0  # Reset patience if model improved\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        # === EARLY STOPPING ===\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24404d-2c09-4736-bbdd-e7c53eec8947",
   "metadata": {},
   "source": [
    "## Experiments and Results\n",
    "#### Load Data, Build Model, Train, and Test\n",
    "`process_and_train()`\n",
    "- Loads dataset from files.\n",
    "- Loads or builds Korean/English tokenizers.\n",
    "- Creates PyTorch DataLoaders.\n",
    "- Instantiates either a Transformer or LSTM+Attention model.\n",
    "- Trains the model and saves the best-performing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39b2bf31-0787-4eb1-a969-9a0c2e2631f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_train(\n",
    "    directory='./translated',\n",
    "    batch_size=64,\n",
    "    d_model=512,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.0003,\n",
    "    num_workers=4,\n",
    "    use_cached_data=True,\n",
    "    subset_fraction=0.5,\n",
    "    model_type=\"transformer\",\n",
    "    model_save_path=\"models\"\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # === LOAD DATA & TOKENIZERS ===\n",
    "    if use_cached_data and os.path.exists('tokenizers/korean_tokenizer.pkl') and os.path.exists('tokenizers/english_tokenizer.pkl'):\n",
    "        print(\"Loading tokenizers from cache...\")\n",
    "        ko_tokenizer = ImprovedKoreanTokenizer.load('tokenizers/korean_tokenizer.pkl')\n",
    "        en_tokenizer = EnglishTokenizer.load('tokenizers/english_tokenizer.pkl')\n",
    "        data = load_excel_files(directory)\n",
    "    else:\n",
    "        data = load_excel_files(directory)\n",
    "        if len(data) == 0:\n",
    "            print(\"No data loaded. Exiting.\")\n",
    "            return None\n",
    "        print(f\"Total data loaded: {len(data)} sentence pairs\")\n",
    "\n",
    "        # Build vocab from scratch\n",
    "        ko_tokenizer = ImprovedKoreanTokenizer(max_vocab=50000)\n",
    "        en_tokenizer = EnglishTokenizer(max_vocab=50000)\n",
    "\n",
    "        print(\"Building Korean vocabulary...\")\n",
    "        ko_tokenizer.fit(data['korean'].tolist())\n",
    "\n",
    "        print(\"Building English vocabulary...\")\n",
    "        en_tokenizer.fit(data['english'].tolist())\n",
    "\n",
    "        os.makedirs('tokenizers', exist_ok=True)\n",
    "        ko_tokenizer.save('tokenizers/korean_tokenizer.pkl')\n",
    "        en_tokenizer.save('tokenizers/english_tokenizer.pkl')\n",
    "        print(\"Tokenizers saved.\")\n",
    "\n",
    "    # === CREATE DATALOADERS ===\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        data, ko_tokenizer, en_tokenizer,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        max_len=30,\n",
    "        pin_memory=(device.type == 'cuda'),\n",
    "        subset_fraction=subset_fraction\n",
    "    )\n",
    "\n",
    "    # === MODEL SELECTION ===\n",
    "    if model_type == \"transformer\":\n",
    "        model = TransformerModel(\n",
    "            src_vocab_size=len(ko_tokenizer.word2idx),\n",
    "            tgt_vocab_size=len(en_tokenizer.word2idx),\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=4,\n",
    "            num_decoder_layers=4,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "\n",
    "    elif model_type == \"lstm\":\n",
    "        INPUT_DIM = len(ko_tokenizer.word2idx)\n",
    "        OUTPUT_DIM = len(en_tokenizer.word2idx)\n",
    "        EMBED_DIM = 256\n",
    "        HIDDEN_DIM = 512\n",
    "        encoder = EncoderRNN(INPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "        decoder = AttentionDecoderRNN(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "        model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "\n",
    "    # === SETUP TRAINING ===\n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input size\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "    # === TRAINING LOOP ===\n",
    "    print(\"\\nStarting model training...\")\n",
    "    best_val_loss = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer, criterion,\n",
    "        ko_tokenizer=ko_tokenizer,\n",
    "        en_tokenizer=en_tokenizer,\n",
    "        num_epochs=num_epochs,\n",
    "        model_save_path=model_save_path\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return model, ko_tokenizer, en_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa37c1-9148-41df-b879-2d02d15ab4ec",
   "metadata": {},
   "source": [
    "#### Entry Point\n",
    "Run the process when this file is executed.\n",
    "\n",
    "- Change the `model_type` to switch between **trandformer** and **LSTM+Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c52ec-11a2-4569-b14c-8cbe40fc0da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined data from cache: ./translated/combined_data_cache.pkl\n",
      "Total data loaded: 1602058 sentence pairs\n",
      "Building Korean vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1602058/1602058 [02:37<00:00, 10162.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n",
      "Building English vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1602058/1602058 [03:57<00:00, 6732.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n",
      "Tokenizers saved.\n",
      "Using 30.0% of the data: 480617 samples\n",
      "Training data: 384493 samples\n",
      "Validation data: 96124 samples\n",
      "Loading cached dataset from dataset_cache/train/cached_data_384493_30.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset from dataset_cache/val/cached_data_96124_30.pkl\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 24031/24031 [33:09<00:00, 12.08it/s, loss=5.91]\n",
      "Evaluating:   0%|          | 0/6008 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Source (KO): 심의회 의 위원장 을 제외 한 위원 은 소속 공무원 , 외부 전문가 로 지명 하 거나 위촉 하 되 , 그 중 2 분 의 1 은 정보\n",
      "Target (EN): the members other than the caused of the college council shall be several if god as affiliated public officials and articles middle , and one half of them shall\n",
      "Prediction : <sos> which official of the members shall be transport by the members of the members of the members of the members of the members of the members of the members of the members of the members of the members of the gu shall be the members of the members of the\n",
      "\n",
      "--- Example 2 ---\n",
      "Source (KO): 이 영화 에서 이미지 와 음악 의 결합 은 영화 에서 음악 을 어떻게 활용 해야 하 는지 를 정확 하 게 보여 주 었 습니다 .\n",
      "Target (EN): the spaces of the release and music in this changed technique child how music should be used in settlement .\n",
      "Prediction : <sos> this is because i think that the changed is the most civil of my raised in the world .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 6/6008 [00:04<1:01:14,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 3 ---\n",
      "Source (KO): 가계 부채 증가 율 이 2015 년 과 2016 년 10 ∼ 11 % 수준 이 었 던 것 과 비교 하 면 속도 가 줄 었 지만 명목\n",
      "Target (EN): the growth rate of recognized debt , not ships at between and percent in and , has slowed down , but it is still high science to the mw\n",
      "Prediction : <sos> in the past years of the year , it was a lot of time in the past years , but it was an headquarters for years .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|▎         | 200/6008 [00:08<04:14, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU score on validation set: 0.0336\n",
      "\n",
      "Epoch 1/20 | Time: 1997.90s\n",
      "Train Loss: 5.9059 | Val Loss: 5.4147\n",
      "✅ Saved new best model (val loss 5.4147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 23024/24031 [31:53<01:21, 12.30it/s, loss=5.24]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  18%|█▊        | 4232/24031 [05:55<30:49, 10.71it/s, loss=4.94]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  99%|█████████▉| 23860/24031 [32:45<00:13, 12.41it/s, loss=4.82]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  22%|██▏       | 5391/24031 [07:26<24:10, 12.85it/s, loss=4.59]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  97%|█████████▋| 23428/24031 [31:59<00:48, 12.42it/s, loss=4.53]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  22%|██▏       | 5297/24031 [07:23<24:19, 12.83it/s, loss=4.32]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  59%|█████▉    | 14157/24031 [19:40<13:24, 12.27it/s, loss=4.31]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  82%|████████▏ | 19737/24031 [27:22<05:50, 12.24it/s, loss=4.3]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  13%|█▎        | 3139/24031 [04:21<28:47, 12.10it/s, loss=4.13]]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 24031/24031 [33:22<00:00, 12.00it/s, loss=4.12]\n",
      "Evaluating:   0%|          | 0/6008 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Source (KO): 심의회 의 위원장 을 제외 한 위원 은 소속 공무원 , 외부 전문가 로 지명 하 거나 위촉 하 되 , 그 중 2 분 의 1 은 정보\n",
      "Target (EN): the members other than the caused of the college council shall be several if god as affiliated public officials and articles middle , and one half of them shall\n",
      "Prediction : <sos> members shall be transport if god by the caused from among the members of the college council , and the members shall be transport if god by the caused from among the members of the college council , and the members shall be transport by the caused from among the\n",
      "\n",
      "--- Example 2 ---\n",
      "Source (KO): 이 영화 에서 이미지 와 음악 의 결합 은 영화 에서 음악 을 어떻게 활용 해야 하 는지 를 정확 하 게 보여 주 었 습니다 .\n",
      "Target (EN): the spaces of the release and music in this changed technique child how music should be used in settlement .\n",
      "Prediction : <sos> the music inspection users how to use the changed and the music release of music in the changed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 6/6008 [00:05<1:05:22,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 3 ---\n",
      "Source (KO): 가계 부채 증가 율 이 2015 년 과 2016 년 10 ∼ 11 % 수준 이 었 던 것 과 비교 하 면 속도 가 줄 었 지만 명목\n",
      "Target (EN): the growth rate of recognized debt , not ships at between and percent in and , has slowed down , but it is still high science to the mw\n",
      "Prediction : <sos> although it was a rate of increase in recognized debt , the increase in the number of recognized debt increased by science to the year , and the increase rate of recognized debt increased . science to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|▎         | 200/6008 [00:09<04:28, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU score on validation set: 0.0799\n",
      "\n",
      "Epoch 6/20 | Time: 2011.47s\n",
      "Train Loss: 4.1170 | Val Loss: 4.1875\n",
      "✅ Saved new best model (val loss 4.1875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▊   | 16483/24031 [23:08<11:53, 10.58it/s, loss=3.94]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  92%|█████████▏| 22197/24031 [30:58<02:29, 12.26it/s, loss=3.94]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  24%|██▍       | 5774/24031 [08:07<25:16, 12.04it/s, loss=3.79]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  46%|████▋     | 11173/24031 [15:33<17:51, 12.00it/s, loss=3.8]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  26%|██▌       | 6253/24031 [08:29<24:32, 12.08it/s, loss=3.67]]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  47%|████▋     | 11359/24031 [15:35<17:03, 12.38it/s, loss=3.69]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  12%|█▏        | 2975/24031 [04:10<31:54, 11.00it/s, loss=3.54] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  36%|███▋      | 8742/24031 [12:11<21:41, 11.75it/s, loss=3.56]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  71%|███████   | 17027/24031 [23:47<09:40, 12.07it/s, loss=3.58]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  97%|█████████▋| 23285/24031 [32:34<01:01, 12.09it/s, loss=3.59]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  16%|█▋        | 3936/24031 [05:32<27:51, 12.02it/s, loss=3.45]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  58%|█████▊    | 13924/24031 [19:32<13:41, 12.31it/s, loss=3.48]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training:  76%|███████▌  | 18199/24031 [25:35<08:04, 12.03it/s, loss=3.49]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, ko_tokenizer, en_tokenizer = process_and_train(\n",
    "        directory='./translated',\n",
    "        use_cached_data=False,\n",
    "        batch_size=16,             \n",
    "        num_epochs=20,\n",
    "        d_model=512,\n",
    "        learning_rate=0.00025,\n",
    "        num_workers=2,              \n",
    "        subset_fraction=0.3,\n",
    "        model_type=\"transformer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f95ed3-92c4-4d4e-b8ef-e24d8b943129",
   "metadata": {},
   "source": [
    "#### RESULTS\n",
    "\n",
    "We were unable to fully complete the training of our beam search translation model due to repeated kernel crashes—primarily caused by out-of-memory (OOM) errors during training on limited hardware.\n",
    "\n",
    "Despite these interruptions, we observed promising improvements in early epochs:\n",
    "- Epoch 1: Validation loss = 5.415, BLEU score = 0.0336\n",
    "- Epoch 6: Validation loss dropped to 4.188, BLEU score improved to 0.0799\n",
    "\n",
    "This upward trajectory suggests that with greater memory and a more powerful GPU, the beam search model has strong potential to continue improving and converge effectively.\n",
    "\n",
    "We remain optimistic about resuming and finalizing training under better computational conditions."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-4.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-4:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
