{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridging Languages with Deep Learning: Building a Korean-English Translator\n",
    "**By Eunice Tu & Yewon Park**\n",
    "\n",
    "In this project, we build a Neural Machine Translation (NMT) system to translate Korean text into English using deep learning models. We experiment with two main architectures: an LSTM-based Sequence-to-Sequence (Seq2Seq) model with attention, and a Transformer-based model. Our aim is to develop models that produce fluent, accurate translations that outperform traditional methods.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Preprocessing\n",
    "#### Data Source:\n",
    "We used the AI Hub Korean-English Parallel Corpus, a professionally curated dataset provided by the Korean government. It contains over one million aligned Korean-English sentence pairs across diverse domains such as news articles, spoken conversations, legal documents, IT, and patents. This dataset is well-suited for our translation project because:\n",
    "\n",
    "- It provides clean, high-quality translations covering both formal and informal language.\n",
    "\n",
    "- It offers sufficient volume to train deep learning models effectively.\n",
    "\n",
    "- It reflects a variety of real-world contexts, improving the model's generalization ability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from konlpy.tag import Mecab\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Configuration\n",
    "Configures the computing environment to guarantees consistent results across different runs:\n",
    "\n",
    "- Checks for GPU availability with PyTorch and sets the device accordingly\n",
    "- Sets random seeds for PyTorch, NumPy, and Python's random module to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initial setup, device configuration, and random seed settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "\n",
    "To ensure that the input data is clean, standardized, and properly formatted for efficient model training, we applied the following preprocessing steps:\n",
    "\n",
    "- **Normalization:**  \n",
    "  We first converted all text to lowercase and standardized spacing by adding spaces around punctuation marks such as `.`, `,`, `?`, and `!`. Non-alphabetic characters (except essential punctuation) were removed, and any multiple consecutive spaces were collapsed into a single space. For robustness, the function also returns an empty string for non-string inputs, ensuring that edge cases are handled appropriately.\n",
    "\n",
    "- **Tokenization:**  \n",
    "  After normalization, we applied a simple word-level tokenizer. Each unique word is assigned a unique index, and special tokens — `<pad>`, `<sos>`, `<eos>`, and `<unk>` — were included to manage padding, sequence start, sequence end, and unknown words respectively.\n",
    "\n",
    "- **Sequence Preparation:**  \n",
    "  Each sentence was wrapped with `<sos>` (start of sentence) and `<eos>` (end of sentence) tokens. Sequences were either padded or truncated to a fixed maximum length, allowing for efficient batching during model training.\n",
    "\n",
    "- **Data Splitting:**  \n",
    "  Currently, the entire dataset is being used for model training. In future work, we plan to split the dataset into training, validation, and test sets to better evaluate the model’s generalization performance.\n",
    "\n",
    "These preprocessing steps collectively transform raw text into a structured format that is clean, consistent, and optimized for training effective machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english(sentence):\n",
    "    \"\"\"Clean English: lowercase, remove symbols, keep letters.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[^a-zA-Z?.!,\\s]', '', sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_korean(sentence):\n",
    "    \"\"\"For Korean, just trim extra spaces (DO NOT strip Hangul).\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.strip()\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer Class\n",
    "The ImprovedTokenizer class provides a custom way to convert text into token IDs and back, enabling consistent text preprocessing and decoding.:\n",
    "\n",
    "- Initialized with a vocabulary size limit \n",
    "- Contains special tokens: `<pad>`(0), `<sos>`(1), `<eos>`(2), and `<unk>`(3)\n",
    "- Builds vocabulary based on word frequency in the training data\n",
    "- The most common words are included in the vocabulary up to the max limit\n",
    "- Provides methods to encode sentences into token IDs and decode IDs back to text\n",
    "- Implements save/load functionality for persistence between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImprovedKoreanTokenizer`\n",
    "\n",
    "This tokenizer provides character-level support for Korean text:\n",
    "\n",
    "- The `tokenize()` method uses `mecab.morphs()` to properly segment Korean text into morphological units\\\n",
    "    **Morphological units (or morphemes) are the smallest linguistic units in a language that have meaning or grammatical function.**\n",
    "- During vocabulary building, it counts frequency of each morphological unit\n",
    "- Words are sorted by frequency and limited to the maximum vocabulary size\n",
    "- The `encode()` method converts Korean text into token IDs using the built vocabulary\n",
    "- The `decode()` method converts token IDs back into readable Korean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Mecab tokenizer for Korean\n",
    "mecab = Mecab()\n",
    "def tokenize_korean(text):\n",
    "    return mecab.morphs(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Morphological Analysis Matters for Korean\n",
    "The ImprovedKoreanTokenizer uses MeCab (through the mecab.morphs() function) to perform morphological analysis because:\n",
    "\n",
    "- Korean is an agglutinative language where numerous grammatical elements attach to word stems\n",
    "- Simply splitting by spaces would be ineffective as Korean sentences often have many morphemes within a single space-separated \"word\"\n",
    "- Character-level splitting would lose the semantic meaning carried by morphological units\n",
    "- Proper morphological segmentation provides much better input for NLP tasks like translation or sentiment analysis\n",
    "\n",
    "This is why the Korean tokenizer needs specialized processing while the English tokenizer can rely on simpler space-based splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedKoreanTokenizer:\n",
    "    \"\"\"Korean-specific tokenizer with character-level support\"\"\"\n",
    "    def __init__(self, max_vocab=50000):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.word_freq = {}\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"Tokenize Korean text appropriately\"\"\"\n",
    "        return tokenize_korean(sentence)\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences\"\"\"\n",
    "        for sent in tqdm(sentences, desc=\"Building vocabulary\"):\n",
    "            for word in self.tokenize(sent):\n",
    "                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Sort by frequency and limit vocab size\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_limit = min(len(sorted_words), self.max_vocab - 4)\n",
    "\n",
    "        idx = 4\n",
    "        for word, _ in sorted_words[:vocab_limit]:\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        \"\"\"Convert sentence to token IDs\"\"\"\n",
    "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in self.tokenize(sentence)]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        return ' '.join([self.idx2word.get(idx, \"<unk>\") for idx in indices if idx != self.word2idx[\"<pad>\"]])\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer state\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load tokenizer state\"\"\"\n",
    "        obj = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            obj.__dict__.update(pickle.load(f))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer for English\"\"\"\n",
    "    def __init__(self, max_vocab=30000):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.word_freq = {}\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"Split English text into words\"\"\"\n",
    "        return sentence.split()\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        for sent in tqdm(sentences, desc=\"Building vocabulary\"):\n",
    "            for word in self.tokenize(sent):\n",
    "                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n",
    "\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_limit = min(len(sorted_words), self.max_vocab - 4)\n",
    "\n",
    "        idx = 4\n",
    "        for word, _ in sorted_words[:vocab_limit]:\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in self.tokenize(sentence)]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(idx, \"<unk>\") for idx in indices if idx != self.word2idx[\"<pad>\"]])\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        obj = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            obj.__dict__.update(pickle.load(f))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class with Caching\n",
    "The `CachedTranslationDataset` class is designed to improve training efficiency for machine translation tasks by preprocessing and caching tokenized data.\n",
    "\n",
    "- **Built on PyTorch Dataset:**  \n",
    "  Inherits from `torch.utils.data.Dataset`, making it fully compatible with PyTorch's `DataLoader` for efficient batching and shuffling.\n",
    "\n",
    "- **Caching Mechanism:**  \n",
    "  Preprocesses each data sample once and stores the result in memory, avoiding repeated preprocessing during each epoch and speeding up training.\n",
    "\n",
    "- **Tokenization and Sequence Preparation:**  \n",
    "  - Converts source sentences (e.g., Korean) and target sentences (e.g., English) into token ID sequences.\n",
    "  - Adds special tokens like `<sos>` (start of sentence) and `<eos>` (end of sentence).\n",
    "  - Applies padding or truncation to ensure all sequences have a fixed maximum length.\n",
    "\n",
    "- **Attention Mask Creation:**  \n",
    "  Generates attention masks that distinguish between actual tokens and padding tokens, helping the model ignore padded positions during training.\n",
    "\n",
    "- **Model-Ready Outputs:**  \n",
    "  Returns tensors for:\n",
    "  - Source input IDs and attention masks\n",
    "  - Target input IDs and labels  \n",
    "  These tensors are ready to be directly fed into the model for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedTranslationDataset(Dataset):\n",
    "    \"\"\"Dataset with caching for faster loading\"\"\"\n",
    "    def __init__(self, df, src_tokenizer, tgt_tokenizer, max_len=40, cache_dir='dataset_cache'):\n",
    "        self.df = df\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_file = os.path.join(cache_dir, f\"cached_data_{len(df)}_{max_len}.pkl\")\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Try to load from cache, otherwise process data\n",
    "        self.cached_data = self._load_or_create_cache()\n",
    "\n",
    "    def _load_or_create_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            print(f\"Loading cached dataset from {self.cache_file}\")\n",
    "            with open(self.cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        print(\"Creating new dataset cache...\")\n",
    "        cached_data = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df)), desc=\"Processing dataset\"):\n",
    "            src_text = self.df.iloc[idx]['korean']\n",
    "            tgt_text = self.df.iloc[idx]['english']\n",
    "            \n",
    "            src_seq = [1] + self.src_tokenizer.encode(src_text) + [2]  # <sos> + sentence + <eos>\n",
    "            tgt_seq = [1] + self.tgt_tokenizer.encode(tgt_text) + [2]  # <sos> + sentence + <eos>\n",
    "            \n",
    "            # Truncate sequences to max_len\n",
    "            src_seq = src_seq[:self.max_len]\n",
    "            tgt_seq = tgt_seq[:self.max_len]\n",
    "            \n",
    "            # Create source and target masks\n",
    "            src_mask = [1] * len(src_seq) + [0] * (self.max_len - len(src_seq))\n",
    "            tgt_mask = [1] * len(tgt_seq) + [0] * (self.max_len - len(tgt_seq))\n",
    "            \n",
    "            # Pad sequences\n",
    "            src_seq += [0] * (self.max_len - len(src_seq))\n",
    "            tgt_seq += [0] * (self.max_len - len(tgt_seq))\n",
    "            \n",
    "            cached_data.append({\n",
    "                'src': torch.tensor(src_seq, dtype=torch.long),\n",
    "                'tgt': torch.tensor(tgt_seq, dtype=torch.long),\n",
    "                'src_mask': torch.tensor(src_mask, dtype=torch.bool),\n",
    "                'tgt_mask': torch.tensor(tgt_mask, dtype=torch.bool),\n",
    "                'src_len': len(src_seq),\n",
    "                'tgt_len': len(tgt_seq)\n",
    "            })\n",
    "        \n",
    "        # Save to cache\n",
    "        print(f\"Saving dataset cache to {self.cache_file}\")\n",
    "        with open(self.cache_file, 'wb') as f:\n",
    "            pickle.dump(cached_data, f)\n",
    "        \n",
    "        return cached_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cached_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading Functions\n",
    "`load_excel_files`:\n",
    "- combines all Excel files from a specified directory into a single DataFrame \n",
    "- automatically identifies the Korean and English columns, preprocesses the text\n",
    "- caches the combined DataFrame for faster reloading in future runs.\n",
    "\n",
    "`create_dataloaders`:\n",
    "- Splits the dataset into training and validation sets\n",
    "- Allows optional subsampling to use only a fraction of the available data if needed. Finally \n",
    "- Creates PyTorch `DataLoader` objects to efficiently batch and feed the data to the model during training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(directory='./translated', pattern=\"*translated.xlsx\"):\n",
    "    \"\"\"Load and combine all Excel files matching the pattern\"\"\"\n",
    "    all_data = []\n",
    "    cache_file = os.path.join(directory, \"combined_data_cache.pkl\")\n",
    "    \n",
    "    # Try to load from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading combined data from cache: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    files = list(Path(directory).glob(pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files matching pattern '{pattern}' found in directory '{directory}'\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(files)} files matching the pattern\")\n",
    "    \n",
    "    for file_path in tqdm(files, desc=\"Loading Excel files\"):\n",
    "        try:\n",
    "            # Attempt to identify Korean and English columns based on common patterns\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Try to automatically detect Korean and English columns\n",
    "            korean_col = None\n",
    "            english_col = None\n",
    "            \n",
    "            # Common column name patterns\n",
    "            korean_patterns = ['korean', 'ko', '한국어', 'source']\n",
    "            english_patterns = ['english', 'en', '영어', 'target']\n",
    "            \n",
    "            # Check column names\n",
    "            for col in df.columns:\n",
    "                col_lower = str(col).lower()\n",
    "                if any(pattern in col_lower for pattern in korean_patterns):\n",
    "                    korean_col = col\n",
    "                if any(pattern in col_lower for pattern in english_patterns):\n",
    "                    english_col = col\n",
    "            \n",
    "            # If automatic detection fails, use the first two columns\n",
    "            if korean_col is None or english_col is None:\n",
    "                if len(df.columns) >= 2:\n",
    "                    korean_col = df.columns[0]\n",
    "                    english_col = df.columns[1]\n",
    "                    print(f\"Using columns: {korean_col} and {english_col} for {file_path.name}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {file_path.name}: Not enough columns\")\n",
    "                    continue\n",
    "            \n",
    "            # Extract and rename columns\n",
    "            file_data = df[[korean_col, english_col]].copy()\n",
    "            file_data.columns = ['korean', 'english']\n",
    "            \n",
    "            # Add source file information\n",
    "            file_data['source_file'] = file_path.name\n",
    "            \n",
    "            # Append to combined data\n",
    "            all_data.append(file_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Clean the data\n",
    "    combined_data = combined_data.dropna()\n",
    "    combined_data['english'] = combined_data['english'].apply(preprocess_english)\n",
    "    combined_data['korean'] = combined_data['korean'].apply(preprocess_korean)\n",
    "    \n",
    "    # Remove rows with too short sentences\n",
    "    combined_data = combined_data[\n",
    "    (combined_data['english'].str.split().str.len() > 3) &\n",
    "    (combined_data['korean'].str.split().str.len() > 3)\n",
    "    ]\n",
    "\n",
    "    # Remove rows with empty strings\n",
    "    combined_data = combined_data[(combined_data['english'] != '') & (combined_data['korean'] != '')]\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving combined data to cache: {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(combined_data, f)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def create_dataloaders(data, ko_tokenizer, en_tokenizer, train_ratio=0.8, \n",
    "                       batch_size=64, max_len=40, num_workers=4, pin_memory=True, \n",
    "                       subset_fraction=0.3):  # Added subset_fraction parameter\n",
    "    \"\"\"Create train and validation DataLoaders with optimized settings\"\"\"\n",
    "    \n",
    "    # Apply subset sampling - NEW\n",
    "    if subset_fraction < 1.0:\n",
    "        sample_size = int(len(data) * subset_fraction)\n",
    "        data = data.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Using {subset_fraction*100}% of the data: {len(data)} samples\")\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    val_data = data.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Training data: {len(train_data)} samples\")\n",
    "    print(f\"Validation data: {len(val_data)} samples\")\n",
    "    \n",
    "    # Create datasets with caching\n",
    "    train_dataset = CachedTranslationDataset(train_data, ko_tokenizer, en_tokenizer, max_len, cache_dir='dataset_cache/train')\n",
    "    val_dataset = CachedTranslationDataset(val_data, ko_tokenizer, en_tokenizer, max_len, cache_dir='dataset_cache/val')\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architectures\n",
    "`PositionalEncoding`:\n",
    "\n",
    "- Adds positional information to word embeddings (because transformer is order-agnostic otherwise).\n",
    "\n",
    "- Uses sine/cosine functions for encoding.\n",
    "\n",
    "`TransformerModel`:\n",
    "\n",
    "- Full Transformer encoder-decoder model.\n",
    "\n",
    "- Embeds input/output sequences + positional encodings.\n",
    "\n",
    "- Defines masking logic for padding and future tokens.\n",
    "\n",
    "- Forward pass to compute outputs for loss calculation.\n",
    "\n",
    "- Includes a translate() method to perform greedy translation (step-by-step prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for the transformer model\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Transformer model for machine translation\"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8, \n",
    "                 num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer architecture\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "        # Model hyper-parameters\n",
    "        self.d_model = d_model\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize model parameters\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        \"\"\"Create masks for transformer\"\"\"\n",
    "        # Source padding mask (for encoder)\n",
    "        src_padding_mask = (src == 0).to(device)\n",
    "        \n",
    "        # Target padding mask (for decoder)\n",
    "        tgt_padding_mask = (tgt == 0).to(device)\n",
    "        \n",
    "        # Target attention mask (for decoder self-attention)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_attention_mask = torch.triu(\n",
    "            torch.ones(tgt_len, tgt_len), diagonal=1\n",
    "        ).bool().to(device)\n",
    "        \n",
    "        return src_padding_mask, tgt_padding_mask, tgt_attention_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        src_padding_mask, tgt_padding_mask, tgt_attention_mask = self.create_mask(src, tgt)\n",
    "        \n",
    "        # Embed source and target sequences\n",
    "        src_embedded = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Apply transformer model\n",
    "        output = self.transformer(\n",
    "            src=src_embedded,\n",
    "            tgt=tgt_embedded,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Apply output layer\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def translate(self, src, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "        \"\"\"Translate a source sentence\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Preprocess source sentence\n",
    "            if isinstance(src, str):\n",
    "                src_tokens = [1] + src_tokenizer.encode(src) + [2]  # <sos> + sentence + <eos>\n",
    "                src = torch.tensor([src_tokens]).to(device)\n",
    "            \n",
    "            # Initialize target with <sos> token\n",
    "            tgt = torch.tensor([[1]]).to(device)  # <sos> token\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                # Generate prediction\n",
    "                src_padding_mask, tgt_padding_mask, tgt_attention_mask = self.create_mask(src, tgt)\n",
    "                \n",
    "                # Embed source and target sequences\n",
    "                src_embedded = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "                tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "                \n",
    "                # Apply transformer model\n",
    "                output = self.transformer(\n",
    "                    src=src_embedded,\n",
    "                    tgt=tgt_embedded,\n",
    "                    src_key_padding_mask=src_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_padding_mask,\n",
    "                    memory_key_padding_mask=src_padding_mask,\n",
    "                    tgt_mask=tgt_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Apply output layer and get next token prediction\n",
    "                output = self.output_layer(output)\n",
    "                next_token = output[:, -1].argmax(dim=1).unsqueeze(1)\n",
    "                \n",
    "                # Append to target sequence\n",
    "                tgt = torch.cat([tgt, next_token], dim=1)\n",
    "                \n",
    "                # Stop if <eos> token is generated\n",
    "                if next_token.item() == 2:\n",
    "                    break\n",
    "            \n",
    "            # Convert token IDs to sentence\n",
    "            output_tokens = tgt.squeeze().tolist()\n",
    "            translated = tgt_tokenizer.decode(output_tokens)\n",
    "            \n",
    "            return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + Attention-based Seq2Seq\n",
    "`EncoderRNN` (LSTM Encoder):\n",
    "\n",
    "- Takes the input (source language) sequence, embeds the tokens into dense vectors, and passes them through an LSTM network to capture contextual information.\n",
    "- Outputs both the full sequence of hidden states (for attention) and the final hidden and cell states (for initializing the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"LSTM Encoder\"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AttentionDecoderRNN` (LSTM Decoder with Attention):\n",
    "- At each decoding step, it uses an attention mechanism to dynamically focus on different parts of the encoder's hidden states.\n",
    "- Combines the current embedded decoder input with the attention context, processes it through an LSTM, and predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"LSTM Decoder with Attention\"\"\"\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = nn.Linear(hidden_dim + embed_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim + embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        hidden_broadcast = hidden[-1].unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        attn_weights = torch.bmm(hidden_broadcast, encoder_outputs.transpose(1, 2))  # (batch_size, 1, seq_len)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Context vector\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Concatenate context and embedding\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, hidden_dim + embed_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Seq2SeqModel` (Seq2Seq Wrapper):\n",
    "- A wrapper that connects the encoder and decoder together into a full translation model.\n",
    "- Defines the overall sequence-to-sequence forward pass, starting by encoding the source sequence and then decoding the target sequence one token at a time using teacher forcing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    \"\"\"Wrapper for LSTM Encoder-Decoder with Attention\"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            input = tgt[:, t]  # Teacher forcing: next input is ground truth\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Functions\n",
    "`train_epoch`:\n",
    "\n",
    "- Train model on one full epoch.\n",
    "\n",
    "- Applies teacher forcing during training (uses ground-truth inputs).\n",
    "\n",
    "- Clips gradients to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]  # Inputs for teacher forcing\n",
    "        tgt_output = tgt[:, 1:]  # Expected outputs\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (progress_bar.n + 1)})\n",
    "    \n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate`:\n",
    "\n",
    "- Evaluate model performance on validation data without updating weights.\n",
    "\n",
    "**Function Workflow**:\n",
    "\n",
    "- Set the model to evaluation mode (`model.eval()`).\n",
    "- Initialize variables for total loss (`epoch_loss`) and BLEU scores (`bleu_scores`).\n",
    "- Loop through validation data:\n",
    "  - Make predictions with the model.\n",
    "  - Compute loss by comparing predictions to target values.\n",
    "  - Calculate **BLEU score** to evaluate prediction quality.\n",
    "  - Optionally print translation examples for inspection.\n",
    "\n",
    "**BLEU Score**:\n",
    "- **Purpose**: BLEU (Bilingual Evaluation Understudy) measures the quality of machine-generated text by comparing n-grams (sequences of words) in the predicted output with those in reference translations.\n",
    "- **Key Features**:\n",
    "  - **N-gram Precision**: It checks how many n-grams (e.g., unigrams, bigrams) from the predicted sentence match those in the reference sentence.\n",
    "  - **Brevity Penalty**: A penalty is applied if the predicted output is shorter than the reference, encouraging the model to produce more complete translations.\n",
    "  - **Smoothing**: Smoothing is used to adjust BLEU calculations in cases where the model produces rare n-grams not present in the reference translations, ensuring more stable scores.\n",
    "- **Usage**: BLEU is especially useful in tasks like machine translation, where we want to evaluate how well a machine-generated translation matches human-produced translations.\n",
    "- **Limitations**: BLEU focuses on surface-level n-gram overlap and doesn't capture the full meaning or fluency of a sentence, so it's less effective for evaluating overall sentence quality or semantic accuracy.\n",
    "  \n",
    "**Output**:\n",
    "- Returns the average loss and average BLEU score across the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, ko_tokenizer=None, en_tokenizer=None, print_examples=3):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    bleu_scores = []\n",
    "    \n",
    "    smooth = SmoothingFunction().method4  # BLEU smoothing\n",
    "\n",
    "    examples_printed = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            output_flat = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt_output_flat = tgt_output.contiguous().view(-1)\n",
    "            loss = criterion(output_flat, tgt_output_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # BLEU evaluation\n",
    "            pred_ids = output.argmax(dim=-1).tolist()\n",
    "            tgt_ids = tgt.tolist()\n",
    "            for pred, tgt_ref in zip(pred_ids, tgt_ids):\n",
    "                # Remove <pad> and special tokens\n",
    "                pred_sentence = [w for w in pred if w not in [0, 1, 2]]\n",
    "                tgt_sentence = [w for w in tgt_ref if w not in [0, 1, 2]]\n",
    "                \n",
    "                if tgt_sentence:\n",
    "                    bleu = sentence_bleu(\n",
    "                        [tgt_sentence], pred_sentence, smoothing_function=smooth\n",
    "                    )\n",
    "                    bleu_scores.append(bleu)\n",
    "\n",
    "                # Print a few examples\n",
    "                if examples_printed < print_examples and ko_tokenizer and en_tokenizer:\n",
    "                    src_tokens = [w for w in batch['src'][examples_printed].tolist() if w not in [0, 1, 2]]  # remove <pad>, <sos>, <eos>\n",
    "                    src_text = ko_tokenizer.decode(src_tokens)\n",
    "                    tgt_text = en_tokenizer.decode(tgt_sentence)\n",
    "                    pred_text = en_tokenizer.decode(pred_sentence)\n",
    "                    print(f\"\\n--- Example {examples_printed+1} ---\")\n",
    "                    print(f\"Source (KO): {src_text}\")\n",
    "                    print(f\"Target (EN): {tgt_text}\")\n",
    "                    print(f\"Prediction : {pred_text}\")\n",
    "                    examples_printed += 1\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    print(f\"\\nAverage BLEU score on validation set: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return epoch_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_model`:\n",
    "\n",
    "- Trains model across multiple epochs.\n",
    "\n",
    "- Saves the best-performing model (lowest validation loss).\n",
    "\n",
    "- Applies early stopping if validation loss doesn’t improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "                ko_tokenizer=None, en_tokenizer=None,\n",
    "                num_epochs=3, patience=2, model_save_path='models'):\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss = evaluate(model, val_loader, criterion, ko_tokenizer, en_tokenizer)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(model_save_path, 'best_model.pt'))\n",
    "            print(f\"Saved new best model with validation loss {val_loss:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full process_and_train() Function\n",
    "Load Data, Build Model, Train, and Test\n",
    "\n",
    "- Load data\n",
    "- Build tokenizers\n",
    "- Create DataLoaders\n",
    "- Choose Transformer or LSTM+Attention\n",
    "- Train and test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_train(directory='./translated', batch_size=64, d_model=256, num_epochs=3, learning_rate=0.0003, num_workers=4, use_cached_data=True, subset_fraction=0.3, model_type=\"transformer\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Load or create tokenizers\n",
    "    if use_cached_data and os.path.exists('tokenizers/korean_tokenizer.pkl') and os.path.exists('tokenizers/english_tokenizer.pkl'):\n",
    "        print(\"Loading tokenizers from cache...\")\n",
    "        ko_tokenizer = ImprovedKoreanTokenizer.load('tokenizers/korean_tokenizer.pkl')\n",
    "        en_tokenizer = EnglishTokenizer.load('tokenizers/english_tokenizer.pkl')\n",
    "        data = load_excel_files(directory)\n",
    "    else:\n",
    "        data = load_excel_files(directory)\n",
    "        if len(data) == 0:\n",
    "            print(\"No data loaded. Exiting.\")\n",
    "            return None\n",
    "        print(f\"Total data loaded: {len(data)} sentence pairs\")\n",
    "        ko_tokenizer = ImprovedKoreanTokenizer(max_vocab=50000)\n",
    "        en_tokenizer = EnglishTokenizer(max_vocab=50000)\n",
    "        print(\"Building Korean vocabulary...\")\n",
    "        ko_tokenizer.fit(data['korean'].tolist())\n",
    "        print(\"Building English vocabulary...\")\n",
    "        en_tokenizer.fit(data['english'].tolist())\n",
    "        os.makedirs('tokenizers', exist_ok=True)\n",
    "        ko_tokenizer.save('tokenizers/korean_tokenizer.pkl')\n",
    "        en_tokenizer.save('tokenizers/english_tokenizer.pkl')\n",
    "        print(\"Tokenizers saved.\")\n",
    "    \n",
    "    # Step 2: Create DataLoaders\n",
    "    train_loader, val_loader = create_dataloaders(data, ko_tokenizer, en_tokenizer, batch_size=batch_size, num_workers=num_workers, pin_memory=(device.type == 'cuda'), subset_fraction=subset_fraction)\n",
    "    \n",
    "    # Step 3: Build Model\n",
    "    if model_type == \"transformer\":\n",
    "        model = TransformerModel(\n",
    "            src_vocab_size=len(ko_tokenizer.word2idx),\n",
    "            tgt_vocab_size=len(en_tokenizer.word2idx),\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=4,\n",
    "            num_decoder_layers=4,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "    elif model_type == \"lstm\":\n",
    "        INPUT_DIM = len(ko_tokenizer.word2idx)\n",
    "        OUTPUT_DIM = len(en_tokenizer.word2idx)\n",
    "        EMBED_DIM = 256\n",
    "        HIDDEN_DIM = 512\n",
    "        encoder = EncoderRNN(INPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "        decoder = AttentionDecoderRNN(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "        model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Step 4: Optimizer and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.05) # Add label smoothing \n",
    "    \n",
    "    # Step 5: Train\n",
    "    print(\"\\nStarting model training...\")\n",
    "    best_val_loss = train_model(model, train_loader, val_loader,\n",
    "        optimizer, criterion, ko_tokenizer=ko_tokenizer,\n",
    "        en_tokenizer=en_tokenizer, num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, ko_tokenizer, en_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entry Point\n",
    "Run the process when this file is executed.\n",
    "\n",
    "- Change the `model_type` to switch between **trandformer** and **LSTM+Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined data from cache: ./translated/combined_data_cache.pkl\n",
      "Total data loaded: 1602058 sentence pairs\n",
      "Building Korean vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1602058/1602058 [00:50<00:00, 31831.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n",
      "Building English vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1602058/1602058 [00:04<00:00, 353998.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n",
      "Tokenizers saved.\n",
      "Using 30.0% of the data: 480617 samples\n",
      "Training data: 384493 samples\n",
      "Validation data: 96124 samples\n",
      "Creating new dataset cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 384493/384493 [00:28<00:00, 13452.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset cache to dataset_cache/train/cached_data_384493_40.pkl\n",
      "Creating new dataset cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 96124/96124 [00:07<00:00, 13621.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset cache to dataset_cache/val/cached_data_96124_40.pkl\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6008/6008 [3:49:39<00:00,  2.29s/it, loss=5.14]     \n",
      "Evaluating:   0%|          | 1/1502 [00:00<14:32,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Source (KO): 겸재 는 금강산 을 오가 는 길 에 이 일대 에 은거 하 던 스승 <unk> <unk> 을 찾아왔 다가 이 폭포 의 경관 에 반해 진경산수 화 를 남겼 다 .\n",
      "Target (EN): on his way to and from mt . geumgang , gyeomjae visited his teacher , <unk> kim <unk> , who had been living in the area , and left real landscape painting , falling in love the view of\n",
      "Prediction : the the way to the <unk> the . <unk> , the was the house in and , <unk> , and was been in in the north , and was the life . . and into the . sea of\n",
      "\n",
      "--- Example 2 ---\n",
      "Source (KO): 구글 은 국내 차량 3 대 중 2 대 의 점유 율 을 자랑 하 는 현대 기아차 에 안드로이드 오토 를 탑재 하 면서 자연스레 IVI 시장 에서 우위 를 점할 수 있 는 토대 가 마련 됐\n",
      "Target (EN): this is because google has laid the foundation for its dominance in the ivi market naturally as it is equipped with android auto to hyundaikia , which boasts a share of two of the three vehicles in korea .\n",
      "Prediction : the is a the has a off largest for its own of the domestic of , , it has a with a <unk> engine the motors which is a large of its cars its worlds major . the .\n",
      "\n",
      "--- Example 3 ---\n",
      "Source (KO): 미 이민 연구 센터 는 매년 3 만 6 천 명 의 원정 출산 여성 이 미국 에서 아이 를 낳 고 있 다는 연구 결과 를 2015 년 에 내놓 은 바 있 다 .\n",
      "Target (EN): the u . s . center for immigration studies released a study in that included findings saying that , women come to the united states to give birth every year in birth tourism schemes .\n",
      "Prediction : the u . s . medical has the education is by survey on the the women of that the the in to the united states . the up to year . the . . . . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1502/1502 [14:58<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU score on validation set: 0.0463\n",
      "Epoch 1/2 | Time: 14678.55s\n",
      "Train Loss: 5.1388 | Val Loss: 4.3817\n",
      "Saved new best model with validation loss 4.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6008/6008 [7:10:37<00:00,  4.30s/it, loss=4.03]     \n",
      "Evaluating:   0%|          | 1/1502 [00:00<08:07,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Source (KO): 겸재 는 금강산 을 오가 는 길 에 이 일대 에 은거 하 던 스승 <unk> <unk> 을 찾아왔 다가 이 폭포 의 경관 에 반해 진경산수 화 를 남겼 다 .\n",
      "Target (EN): on his way to and from mt . geumgang , gyeomjae visited his teacher , <unk> kim <unk> , who had been living in the area , and left real landscape painting , falling in love the view of\n",
      "Prediction : at the way to the left the . kumgang , the , the tomb , who , <unk> , who was been on in the mountains of and the the time . , was into the . mountains of\n",
      "\n",
      "--- Example 2 ---\n",
      "Source (KO): 구글 은 국내 차량 3 대 중 2 대 의 점유 율 을 자랑 하 는 현대 기아차 에 안드로이드 오토 를 탑재 하 면서 자연스레 IVI 시장 에서 우위 를 점할 수 있 는 토대 가 마련 됐\n",
      "Target (EN): this is because google has laid the foundation for its dominance in the ivi market naturally as it is equipped with android auto to hyundaikia , which boasts a share of two of the three vehicles in korea .\n",
      "Prediction : the is the the has been the base for its own of the domestic of , in it has equipped with a motors vehicles maximize motors which has its competitive of the cars the domestic major , korea .\n",
      "\n",
      "--- Example 3 ---\n",
      "Source (KO): 미 이민 연구 센터 는 매년 3 만 6 천 명 의 원정 출산 여성 이 미국 에서 아이 를 낳 고 있 다는 연구 결과 를 2015 년 에 내놓 은 바 있 다 .\n",
      "Target (EN): the u . s . center for immigration studies released a study in that included findings saying that , women come to the united states to give birth every year in birth tourism schemes .\n",
      "Prediction : according u . s . womens has women studies has a survey that , women women of that the children have to the u states every the birth to year . the . . . . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1502/1502 [08:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU score on validation set: 0.0773\n",
      "Epoch 2/2 | Time: 26332.46s\n",
      "Train Loss: 4.0288 | Val Loss: 3.7120\n",
      "Saved new best model with validation loss 3.7120\n",
      "\n",
      "Training completed in 41150.55 seconds\n",
      "Best validation loss: 3.7120\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, ko_tokenizer, en_tokenizer = process_and_train(\n",
    "        directory='./translated',\n",
    "        use_cached_data=False,\n",
    "        batch_size=64,\n",
    "        num_epochs=2,\n",
    "        d_model=384,\n",
    "        learning_rate=0.00025,\n",
    "        num_workers=0,\n",
    "        subset_fraction=0.3,\n",
    "        model_type=\"transformer\"  # Change to \"lstm\" if you want LSTM+Attention\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined data from cache: ./translated/combined_data_cache.pkl\n",
      "Bad rows sample:\n",
      "Empty DataFrame\n",
      "Columns: [korean, english, source_file]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Reload Excel files\n",
    "data = load_excel_files(directory='./translated')\n",
    "\n",
    "# Check the worst examples BEFORE caching\n",
    "bad_rows = data[\n",
    "    (data['korean'].str.strip().str.len() < 5) | \n",
    "    (data['korean'].str.split().str.len() <= 3)\n",
    "]\n",
    "\n",
    "print(\"Bad rows sample:\")\n",
    "print(bad_rows.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined data from cache: ./translated/combined_data_cache.pkl\n",
      "Using 10.0% of the data: 160228 samples\n",
      "Training data: 128182 samples\n",
      "Validation data: 32046 samples\n",
      "Loading cached dataset from dataset_cache/train/cached_data_128182_40.pkl\n",
      "Loading cached dataset from dataset_cache/val/cached_data_32046_40.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunicetu/anaconda3/envs/torch-clean/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Source (KO): 어제 전화 로 예약 하 고 왔 고요 , 체크인 해 주 세요 .\n",
      "Target (EN): i made a reservation yesterday via phone call , and im here for the checkin .\n",
      "Prediction  : <sos> it is a good idea to see the new relatively times , so shall korea see the new relatively . <eos>\n",
      "\n",
      "--- Example 2 ---\n",
      "Source (KO): 오 는 19 일 까지 KT 채용 홈페이지 를 통해 지원 할 수 있 으며 남자 인 경우 병역 문제 가 해결 돼야 한다 .\n",
      "Target (EN): the application can be made through kts recruitment website by th , and in case of a man , the military service issue must be resolved .\n",
      "Prediction  : <sos> it is not urban to users that the korean wave is not urban , but it is not urban to users the situation . <eos>\n",
      "\n",
      "--- Example 3 ---\n",
      "Source (KO): 사용 허가 를 받 은 다음 사용 시작 전날 까지 미리 그 사용 을 취소 또는 연기 할 때 에 는 총 사용료 의 10 퍼센트 를 공제 후 반환 하 고 , 사용 시작 일 이후 는 이용\n",
      "Target (EN): when the use is cancelled or postponed in advance by the day before the commencement of the use after the permission is granted , the fee shall be refunded after deducting ten percent of the total user fee ,\n",
      "Prediction  : <sos> the medical why the government is not to do anything conversation , but it is not urban to say that it is not urban to say that it is not urban to do . <eos>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data and tokenizers\n",
    "data = load_excel_files(directory='./translated')\n",
    "ko_tokenizer = ImprovedKoreanTokenizer.load('tokenizers/korean_tokenizer.pkl')\n",
    "en_tokenizer = EnglishTokenizer.load('tokenizers/english_tokenizer.pkl')\n",
    "\n",
    "\n",
    "# Step 2: Create val_loader (you don’t need to retrain)\n",
    "_, val_loader = create_dataloaders(\n",
    "    data,\n",
    "    ko_tokenizer,\n",
    "    en_tokenizer,\n",
    "    batch_size=1,  # use batch size 1 for easy viewing\n",
    "    subset_fraction=0.1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Step 3: Load model and weights\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=len(ko_tokenizer.word2idx),\n",
    "    tgt_vocab_size=len(en_tokenizer.word2idx),\n",
    "    d_model=384,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load('models/best_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Print a few translations from the validation set\n",
    "from random import randint\n",
    "\n",
    "for i in range(3):  # print 3 examples\n",
    "    idx = randint(0, len(val_loader.dataset) - 1)\n",
    "    sample = val_loader.dataset[idx]\n",
    "\n",
    "    src_tokens = sample['src'].tolist()\n",
    "    tgt_tokens = sample['tgt'].tolist()\n",
    "\n",
    "    src_sentence = ko_tokenizer.decode([t for t in src_tokens if t not in [0, 1, 2]])\n",
    "    tgt_sentence = en_tokenizer.decode([t for t in tgt_tokens if t not in [0, 1, 2]])\n",
    "\n",
    "    # Get model translation\n",
    "    translation = model.translate(src_sentence, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Source (KO): {src_sentence}\")\n",
    "    print(f\"Target (EN): {tgt_sentence}\")\n",
    "    print(f\"Prediction  : {translation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
